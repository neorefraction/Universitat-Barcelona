{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==============================================================================================\n",
    "# Practicum 5\n",
    "\n",
    "=============================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Delivery\n",
    "\n",
    "Your overall grading will be penalized if the following requirements are not fulfilled:\n",
    "\n",
    "- Implemented code should be commented exhaustively and in **English**. \n",
    "\n",
    "- The questions introduced in the exercises must be answered.\n",
    "\n",
    "- Add title to the figures to explain what is displayed.\n",
    "\n",
    "- Answers to questions also need to be in **English**.\n",
    "\n",
    "- Make sure to print and plot exactly what it is indicated. If a reference image is provided, your output is expected to be exactly the same unless instructed differently. \n",
    "\n",
    "- The deliverable of both parts must be a file named **P5_Student1_Student2.zip** that includes:\n",
    "    - The notebook P5_Student1_Student2.ipynb completed with the solutions to the exercises and their corresponding comments.\n",
    "    - All the images used in this notebook (upload the ones that were not provided)\n",
    "    \n",
    "- It is required that your code can be run by us without need of any modification and without getting any errors.\n",
    "\n",
    "- Use packages and solutions that were covered in your class and tutorials. If you are unsure about using a particular package, you should seek clarification from your instructor to confirm whether it is allowed.\n",
    "\n",
    "- Please refrain from utilizing resources like ChatGPT to complete this lab assignment.\n",
    "\n",
    "\n",
    "**Deadline: December 22th, 23:00 h**\n",
    "\n",
    "==============================================================================================\n",
    "\n",
    "This lab covers the following topics: \n",
    "* Gaussian filters\n",
    "* Descriptors based on texture\n",
    "* Distance between images and similarity search\n",
    "* Face detection using:\n",
    "    * Haar-like features\n",
    "    * Adaboost\n",
    "    * PCA\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==============================================================================================\n",
    "##  Image search using textures\n",
    "==============================================================================================\n",
    "\n",
    "#### Problem we want to solve\n",
    "- Given a query image **$x$** and a set of images **$X$** we would like to retreive the most similar to **$x$** images from  **$X$**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# GENERAL ONES FOR IMAGE ANALYSIS\n",
    "import time\n",
    "import scipy\n",
    "import numpy as np\n",
    "import skimage\n",
    "from skimage import filters\n",
    "from skimage import io\n",
    "from skimage.color import rgb2gray, rgba2rgb\n",
    "from skimage.transform import resize\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from time import time\n",
    "from scipy.ndimage import convolve\n",
    "\n",
    "### FACE DETECTION\n",
    "from skimage.feature import haar_like_feature\n",
    "from skimage.feature import haar_like_feature_coord\n",
    "from skimage.feature import draw_haar_like_feature\n",
    "from skimage.transform import integral_image\n",
    "\n",
    "### FACE RECOGNITION\n",
    "# Load the dataset\n",
    "from sklearn.datasets import fetch_lfw_people \n",
    "# Classification and results\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import classification_report "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Derivative of Gaussians aka Leung-Malik (LM) Filter Bank\n",
    "We can apply a collection of multiple filters that we call a filter bank. Note that if we apply $D$ filters our feature vectors will be $D$ dimensional.\n",
    "\n",
    "The following image shows a filter bank. In the filter bank we typically want filters to capture a combination of scales, orientations of different types of patterns. This particular filter bank is The Derivative-of-Gaussians or as known as Leung-Malik (LM) Filter Bank.\n",
    "\n",
    "<img src=\"./images/filter_bank.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import Leung-Malik filters as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import LM_filters\n",
    "filter_bank = LM_filters.makeLMfilters()\n",
    "# NOTE: If this does not work, go to appendix for function code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting all filters\n",
    "\n",
    "Plot the created filters as images similar to the figure above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Specifies the number of rows and columns for the figure\n",
    "nrows = 4\n",
    "ncols = 12\n",
    "\n",
    "# Creates a figure to plot Leung-Malik filters\n",
    "fig, axs = plt.subplots(nrows, ncols, figsize=(10, 3))\n",
    "# Adds a title to the figure\n",
    "fig.suptitle('Leung-Malik Filters')\n",
    "\n",
    "# Iterates over the 48 filters to display them in a 12 by 12 grid across 4 rows\n",
    "for i in range(nrows):\n",
    "\n",
    "    # Calculates the range to retrieve the filters\n",
    "    _from = i * 12\n",
    "    _to = (i + 1) * 12\n",
    "\n",
    "    # Selects the filters\n",
    "    filters = filter_bank[:,:, _from:_to]\n",
    "\n",
    "    # Adds each filter to its respective row\n",
    "    for j in range(ncols):\n",
    "        axs[i][j].axis('off')\n",
    "        axs[i][j].imshow(filters[:,:,j], cmap='gray')\n",
    "\n",
    "# Displays the generated figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Extracting a feature vector for an image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.1** Read the image ``/images/pizza.jpg``, and resize it to 240ùë•240 pixels. Then, convert the image to grayscale and visualize both, the RGB and the grayscale images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reads the image\n",
    "pizza_rgb = io.imread('images/pizza.jpg')\n",
    "\n",
    "# Resizes the image to 240x240x3\n",
    "pizza_rgb = resize(pizza_rgb, (240, 240, 3))\n",
    "\n",
    "# Converts the image to grayscale\n",
    "pizza_gray = rgb2gray(pizza_rgb)\n",
    "\n",
    "# Stores the images and their labels\n",
    "images = [pizza_rgb, pizza_gray]\n",
    "labels = ['Original Pizza Resized', 'Grayscale Resized Pizza']\n",
    "\n",
    "# Creates a figure to plot the images\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 7))\n",
    "\n",
    "# Plots the images in the figure with their labels\n",
    "for i, (img, lab) in enumerate(zip(images, labels)):\n",
    "\n",
    "    # Sets up the subplot\n",
    "    axs[i].axis('off')\n",
    "    axs[i].set_title(lab)\n",
    "\n",
    "    #¬†Adds the image to the plot\n",
    "    if img.ndim == 2:\n",
    "        axs[i].imshow(img, cmap='gray')\n",
    "    else:\n",
    "        axs[i].imshow(img)\n",
    "\n",
    "# Displays the generated figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2** Given $D=n\\_{filters}$ filters from the filter bank and a single image `image`, make a function `get_image_features(image, filter_bank, n_filters)` that returns a feature vector of shape `n_filters`. The returned vector must contain at position $k$ the **mean of the absolute value of the convolved image** by filter $k$.\n",
    "\n",
    "$$\n",
    "\\text{feat}(x) = \\left( \\text{mean}( |r_1|), \\dots,\\text{mean}(|r_D|) \\right)\n",
    "$$\n",
    "\n",
    "Try the function with the previous image and print the feature vector.\n",
    "\n",
    "**Hint**: The function should return a feature vector obtained by averaging each filter response on the image. Use a small number of filters to try your functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def convert_to_grayscale(image: np.ndarray) -> np.ndarray:\n",
    "    '''\n",
    "    Converts an image to grayscale if it is RGB or RGBA.\n",
    "\n",
    "    Args:\n",
    "        image: The image to be converted.\n",
    "    \n",
    "    Returns:\n",
    "        The image converted to grayscale.\n",
    "    '''\n",
    "\n",
    "    if image.ndim == 3:  # Checks if the image isn't grayscale\n",
    "        if image.shape[2] == 4:  # Checks if it's an RGBA image\n",
    "            image = rgba2rgb(image)\n",
    "\n",
    "        if image.shape[2] == 3:  # Checks if it's an RGB image\n",
    "            image = rgb2gray(image)\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "def get_image_features(image: np.ndarray, filter_bank: np.ndarray, n_filters: int) -> np.ndarray:\n",
    "    '''\n",
    "    Returns the feature vector of an image from a filter bank.\n",
    "\n",
    "    Args:\n",
    "        image: Source image from which to extract features.\n",
    "        filter_bank: Set of filters to apply.\n",
    "        n_filters: Number of filters to be applied from the filter bank.\n",
    "\n",
    "    Returns:\n",
    "        A vector (list) with n_filters elements representing the mean of the absolute image values with the first n_filters filters convolved.\n",
    "    '''\n",
    "\n",
    "    image = convert_to_grayscale(image)  # Ensures the image is in grayscale\n",
    "\n",
    "    # Gets the first n filters of the bank\n",
    "    filters = [filter_bank[:,:,i] for i in range(n_filters)]\n",
    "\n",
    "    # Calculates the feature vector\n",
    "    features = [np.absolute(convolve(image, f)).mean() for f in filters]\n",
    "\n",
    "    return np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Try the function\n",
    "n_filters = 5\n",
    "im_features = get_image_features(pizza_gray, filter_bank, n_filters)\n",
    "im_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**1.3** (Optional) Try ``get_image_features()`` using a different number of filters and other images (for instance ``/images/dog.jpg`` or ``/images/flower.jpg``)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads, resizes and converts the images to grayscale\n",
    "gray_dog = rgb2gray(resize(io.imread('images/dog.jpg'), (250, 250, 3)))\n",
    "gray_flower = rgb2gray(resize(io.imread('images/flower.jpg'), (250, 250, 3)))\n",
    "\n",
    "# Gets the images features vectors\n",
    "dog_features = get_image_features(gray_dog, filter_bank, n_filters)\n",
    "flower_features = get_image_features(gray_flower, filter_bank, n_filters)\n",
    "\n",
    "# Prints each feature vector\n",
    "print(f'Grayscale dog features vector: {dog_features}')\n",
    "print(f'Grayscale flower features vector: {flower_features}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.4** Make a function `visualize_features(im,  filter_bank, n_filters=5)` that receives the `filter_bank`, an image `im` and an integer `n_filters`. \n",
    "\n",
    "The function  must make a plot of two rows containing in the first row, in position $k$, the image convolved by filter $k$. In the second row, in position $k$, the image of the k'th filter. The result for `n_filter=5` should look like \n",
    "\n",
    "\n",
    "<img src=\"./images/filters.png\" >\n",
    "\n",
    "**Hint**: Note that since at this moment we are focusing on the texture, we will not use the color of the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_features(im: np.ndarray, filter_bank: np.ndarray, n_filters: int = 5) -> None:\n",
    "    '''\n",
    "    Displays the result of the convolving an image with the first n Leung-Malik filters.\n",
    "\n",
    "    Args:\n",
    "        im: The 2D array representation of an image.\n",
    "        filter_bank: A 3D matrix containing the Leung-Malik filters.\n",
    "        n_filters: An integer specifying the number of filters to be applied.\n",
    "    '''\n",
    "\n",
    "    im = convert_to_grayscale(im)  # Ensures the image is in grayscale\n",
    "\n",
    "    # Creates a figure to plot the images and filters\n",
    "    fig, axs = plt.subplots(2, n_filters, figsize=(10, 4))\n",
    "\n",
    "    # Gets the first n filters of the bank\n",
    "    filters = [filter_bank[:,:,i] for i in range(n_filters)]\n",
    "\n",
    "    # Adds the images convolved with each filter and the filters to the figure\n",
    "    for i, f in enumerate(filters):\n",
    "        axs[0][i].axis('off')\n",
    "        axs[1][i].axis('off')\n",
    "\n",
    "        axs[0][i].imshow(convolve(im, f), cmap='gray')\n",
    "        axs[1][i].imshow(f, cmap='gray')\n",
    "\n",
    "    # Displays the generated figure\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Try the function\n",
    "visualize_features(pizza_gray, filter_bank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.5** (Optional) Try ``visualize_features()`` using a different number of filters and using another image (for instance ``/images/dog.jpg`` or ``/images/flower.jpg``)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_features(gray_dog, filter_bank, 6)\n",
    "visualize_features(gray_flower, filter_bank, 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Load & resize the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1** Read all the images in the directories, resize them to 240x240 pixels and save the image in an array:\n",
    "\n",
    "<ul>\n",
    "    <li>./images/pizza/</li>\n",
    "    <li>./images/flowers/</li>\n",
    "    <li>./images/pets/</li>\n",
    "</ul>\n",
    "\n",
    "**Hint:** You have to create an array for each directory, which containts all the images belonging to that path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize one image (the first) from each array. Use ``subplot`` to create a 3x1figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many images there are in **each** directory?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing the matrix of the feature vectors for all images\n",
    "\n",
    "**2.2** Implement a function `get_dataset_features(all_images, filter_bank, n_filters=6)` that applies `get_image_features` to get a feature vector for each of the images in the union of the three datasets (fish and chips_images, pizza, paella). It must return a matrix containing at row $k$ feature vector for the input image $k$.\n",
    "\n",
    "Try the function with the whole data set:\n",
    "\n",
    "*all_images = pizza_images + flowers_images + pets_images*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Your solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Try the function\n",
    "all_images = pizza_images + flowers_images + pets_images\n",
    "feature_vectors=get_dataset_features(all_images,  filter_bank, n_filters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the shape of `feature_vectors` as well as the features for image #3 (*i.e. all_images[2]*), image #33, and image #53, directly from the vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the features of an image\n",
    "\n",
    "**2.3** Define a function `visualize_features_imgs(im_index, feature_vector)` that given n different images, plots their features. Use different colors to distinguish the features of each image.\n",
    "Choose 3 images on your choice and visualize the results.\n",
    "\n",
    "<img src=\"./images/example_plot_features.jpg\" >\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Try the function\n",
    "visualize_features_imgs( [2,32,52] , feature_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 3 Retrieving the most similar images\n",
    "\n",
    "### Similarity search\n",
    "\n",
    "Let us assume $f(x) \\in \\mathbb{R}^D$ represents a set of features for $x$. Given a query image $x$ and another image $x^m$ from the database, we can compute the distance between images as\n",
    "$$\n",
    "\\text{distance}\\left( f(x) , \\, f(x^m) \\right) = \\| \\text{feat}(x)  - \\text{feat}(x^m)  \\|_2 =  \\sqrt{ \\sum_{d=1}^\\text{D} \\left( f(x)_d - f(x^m)_d  \\right)^2 }\n",
    "$$\n",
    "\n",
    "then we can find the closest image $x^{m^*}$ from the database to $x$ as $m^* =  \\text{argmin}_{m} \\{ \\| \\text{feat}(x)  - \\text{feat}(x^m)  \\|_2 \\}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**3.1** Implement a function `retrieve_images(im_features, feature_vectors, k=5)` to retrieve and visualize the `k` most similar images (according to the l2 norm) to `im` and the corresponding distances.\n",
    "\n",
    "The input of this function need to be feature of the image of interest `im_features`, the whole data set features `feature_vectors`, and an integer value `k`, which defines the number of images.\n",
    "\n",
    "The function should return the ordered vector of distances and the index of the closests images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try your function using ``/images/pizza.jpg``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try your function\n",
    "distances, closest = retrieve_images(im_features, feature_vectors, k=5)\n",
    "print(closest, distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.2** Write a function *show_closest_images(all_images, closest, distances)* to show the closest images and the corresponding distances. \n",
    "\n",
    "<img src=\"./images/indice.png\" >\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Compute the accuracy of the algorithm: given a query image. \n",
    "\n",
    "**4.1** Implement a function `accuracy(feature_vectors, class_labels, im_features, class_im,  k=5)`, which takes as input `feature_vectors`, the `class_labels` for the images (`0 = pizza`, `1 = flower` and `2 = pet`), the number of images to retrieve `k`, a query image (i.e. the feature vector for the image of interest) and the class of the query image `class_im`. \n",
    "\n",
    "Returns as output the number of retrieved images that belong to class `class_im` divided by the total of images retrieved `k` (this is the accuracy).\n",
    "\n",
    "**Hint**: Be careful to exclude the query image from the retrieved images (i.e. those cases in which `distance=0.0`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"acc:\", accuracy(feature_vectors, class_labels, im_features, 0 , k=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the accuracy with k=10 with the previous images. Does the accuracy match the images retrieved from the previous exercise?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature normalization\n",
    "\n",
    "**4.3** As you can see in exercise 2.4, some features are higher than the rest and, therefore, they are \"more important\" during the image retrieving.\n",
    "\n",
    "Normalize each feature between **[0,1]** using the minimum and the maximum values obtained from `feature_vectors`. Note that we need to use these values to normalize `im_features`.\n",
    "\n",
    "* Plot the new features of an image, using `visualize_features_imgs()`. \n",
    "* Then, show the 5 closest images (use show_closest_images) from `im_features`\n",
    "* Compute the accuracy. \n",
    "\n",
    "**Is there any difference? Do we obtain the same images than previously?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Image retrieval based on texture and color. \n",
    "\n",
    "**5.1** Make a function ` lm_features_rgb(rgb_image, n_bins=12)`  that returns the features based on a color descriptor (using histograms, similar to what it was performed in the **Deliverable 3 - video segmentation**). \n",
    "\n",
    "**Hint:** Consider each bin from each channel as a feature, create a vector with all the features together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try your function\n",
    "lm_rgb_features = lm_features_rgb(pizza_rgb)\n",
    "print(len(lm_rgb_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Apply the function to all the images in the dataset\n",
    "\n",
    "**5.2** Using ` lm_features_rgb` build the features of all the datapoints in  `all_images` and save them in `lm_dataset_features`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "'''\n",
    "Solution\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the accuracy of the method using `lm_features_rgb`\n",
    "\n",
    "**5.3** Compute the accuracy of the retrieved images using the features that contain color information. Show the 4 closest images to the query images in the `lm_rgb` space with the function built before.\n",
    "\n",
    "Is the accuracy higher? Please, comment the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==============================================================================================\n",
    "## Haar-like features applied for face detection\n",
    "=============================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a Haar-like feature?\n",
    "\n",
    "Haar-like feature descriptors were successfully used to implement the first real-time face detector. In this laboratory we will see an example illustrating the extraction, selection, and classification of Haar-like features to detect faces vs. non-faces.\n",
    "\n",
    "Documentation [Haar-like feature skimage](https://scikit-image.org/docs/0.14.x/auto_examples/xx_applications/plot_haar_extraction_selection_classification.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Haar-like features are features extracted from the images to recognize objects. These features are normally used in face recognition. The key to face recognition is to detect the relevant features of humans such as eyes, lips, or nose. \n",
    "\n",
    "\n",
    "<img src=\"images/haar-like.PNG\" width=400, height=400>\n",
    "\n",
    "Try to guess where in the face image we expect to detect an edge, line or another facial feature and what would be the most appropriate Haar-feature for them? \n",
    "\n",
    "<img src=\"images/haar-like1.PNG\" width=500, height=500>\n",
    "\n",
    "\n",
    "A real application would be:\n",
    "\n",
    "<img src=\"images/face.PNG\" width=300, height=300>\n",
    "\n",
    "\n",
    "To describe the face, we can apply convolutions with Haar features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Building integral image and Haar-like features\n",
    "\n",
    "Compute all the Haar-like features (we can define up to 16000 million masks), can be a slow process. To compute it faster, we are going to use the integral images (instead of convolutions). It is very useful because we are able to save all the sums and substrations of image rectangles to avoid computing all the features every time.\n",
    "\n",
    "When creating an Integral Image, we need to create a Summed Area Table. What does represent any point (x,y) in this table?\n",
    "\n",
    "<img src=\"images/integral_image.PNG\" width=250, height=2500>\n",
    "\n",
    "An example :\n",
    "\n",
    "<img src=\"images/integral_image1.PNG\" width=400, height=400>\n",
    "\n",
    "To easy the computation of Haar features, the integral image must have an additional row and column full of zeros (first row and first column). Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your written answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6.1** Build a function `to_integral_image` that computes the integral image of an input (2D) array. The integral image must have an additional row and column full of zeros (first row and first column).\n",
    "Make sure that the values of the integral image are correct.\n",
    "\n",
    "Try your function using a `5x5` random grayscale image. Visualize both the original random image and the integral one. Refrain from using built-in functions that already do the integral, compute it yourself. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing results on a random grayscale image\n",
    "random_img = np.random.rand(5,5)\n",
    "integral_img = to_integral_image(random_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sure that the values of the integral image are correct, compute the following tests:\n",
    "\n",
    " - `img_array.sum() == ii_img_array[-1,-1]`\n",
    " - `img_array[0,:].sum() == ii_img_array[1,-1]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6.2** Let's check in real images. Choose an image from the directory ``./faces``, visualize both the original and the integral image, and make the same test that in the previous question.\n",
    "\n",
    "Here is an example (you don't have to pick this image, you can do it at random):\n",
    "<img src=\"images/integral_image_example.jpg\" width=500, height=250>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6.3:** Alternatively to your own function, you can use the ``integral_image()`` function from ``skimage.transform``. Compare (numerically) the result obtained using your funtion and that obtained using the function provided by skimage (no need to provide a written answer here, just make sure you are obtain same results)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Haar-like features\n",
    "\n",
    "Let's use the [haar_like_feature()](https://scikit-image.org/docs/0.14.x/api/skimage.feature.html#skimage.feature.haar_like_feature) function from skimage. Check the parameters and the returned value of the ``function haar_like_feature()`` before continuing and **NOTE** that we must use the integral image (**not the real image**) in this function.\n",
    "\n",
    "*skimage.feature.haar_like_feature(int_image, rint, cint, widthint, heightint, feature_type=None, feature_coord=None)*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Extract features\n",
    "\n",
    "**6.4** Define a function ``extract_feature_image(image, feature_type, feature_coord=None)`` to obtain the Haar-like features, using a given type of features ``feature_types``, from an image. The aim of this function is as simple as to join both `to_integral_image()` and `haar_like_feature()`functions.\n",
    "\n",
    "Try your function using the choosing image from *1.4*. You should obtain a feature vector. Print the vector shape.\n",
    "\n",
    "**Note:** You can give an array with a set of feature types to the `haar_like_feature()` function and it will compute all the corresponding features. We **do not** need to give each time only one feature.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_types = ['type-2-x', 'type-2-y',\n",
    "                 'type-3-x', 'type-3-y',\n",
    "                 'type-4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6.5** Plot a Haar-like feature on an image:\n",
    "\n",
    "To visualize Haar-like features on an image, we need the fuctions, provided by skimage, ``haar_like_feature_coord()``, which computes the coordinates of Haar-like features, and ``draw_haar_like_feature()``, used to visualize that features.\n",
    "\n",
    "Before continuing, please, **check the online documentation of the two functions**\n",
    "\n",
    "- [*haar_like_feature_coord(width, height, feature_type=None)*](https://scikit-image.org/docs/stable/api/skimage.feature.html#skimage.feature.haar_like_feature_coord)\n",
    "\n",
    "- [*draw_haar_like_feature(image, r, c, width, height, feature_coord, color_positive_block=(1.0, 0.0, 0.0), color_negative_block=(0.0, 1.0, 0.0), alpha=0.5, max_n_features=None, random_state=None)*](https://scikit-image.org/docs/stable/api/skimage.feature.html#skimage.feature.draw_haar_like_feature)\n",
    "\n",
    "Define a function ``plotFeatures``  to visualize Haar-like features on an images, given a array of feature types ``feature_types``. The aim of this exercise is, similarly to the previous one, to merge both `haar_like_feature_coord()` and `draw_haar_like_feature()` functions. Try your own function using the choosing image from *1.4* as follows:\n",
    "\n",
    "<img src=\"images/image1.png\" width=600, height=300>\n",
    "\n",
    "*Note that there are multiple features and therefore your results might not exactly match this example, depending on your selection.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 Face detection using an Adaboost\n",
    "\n",
    "**7.1** Read all the images from the directories ``./faces`` and ``./nonfaces`` and build an array with the all the features (use the `built extract_feature_image` for this). \n",
    "\n",
    "Futhermore, build the class labels vector ``y`` with the label of all the images. Make sure both, images and labels are numpy array objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7.2** Using the ``train_test_split()`` function from `sklearn.model_selection`, divide the dataset into *train* and *test* sets. The test size must be the 30% (i.e. 0.3) of the whole dataset.\n",
    "\n",
    "[*sklearn.model_selection.train_test_split(*arrays, test_size=None, train_size=None, random_state=None, shuffle=True, stratify=None)*](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)\n",
    "\n",
    "Print the size of your images train and test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7.3** Train an Adaboost classifier using `AdaBoostClassifier()`from `sklearn.ensemble`.\n",
    "\n",
    "[*sklearn.ensemble.AdaBoostClassifier(n_estimators=50, learning_rate=1.0)*](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html)\n",
    "\n",
    "What is an Adaboost? How it works?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7.4** Evaluate the accuracy of the Adaboost classifier using the *predict* and *score* methods of the classifier. Make sure to use your test set for this. What are these methods doing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the parameter ``n_estimators`` and see what happens to the test set accuracy. Does the performance increased or decreased?\n",
    "\n",
    "Try at least 3 different values of `n_estimators`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7.5 (Optional)** The method ``feature_importances_`` of the Adaboost is giving the importance of the features. Implement a function to visualize the 10 most important features of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==============================================================================================\n",
    "## Principal Component Analysis (PCA) applied for face recognition\n",
    "==============================================================================================\n",
    "\n",
    "### Dimensionality and redundancy\n",
    "\n",
    "Imagine we have a dataset with 100x100 pixel images, it means we have 10000 dimensions. We want to construct a low-dimensional linear subspace that best explains the variation in the set of face images (**Eigenfaces space**)\n",
    "\n",
    "<img src=\"images/subspace.PNG\" width=200, height=200>\n",
    "\n",
    "Each image has m rows and n columns and defines a vector of (mxn) elements. We need to choose the most valuable pixels in order to avoid compute all dimensions. \n",
    "\n",
    "<img src=\"images/feature_vector.PNG\" width=800, height=400>\n",
    "\n",
    "We look for a transformation of the original space to a smaller (M << (mxn)) where faces are represented with their coordinates in this new space R.\n",
    "\n",
    "To reduce the dimensionality retaining the information necessary to classify and recognize, we are going to use the **Eigenfaces method** \n",
    "\n",
    "### How to build a reduced space?\n",
    "\n",
    "To build this new space, we are going to use the **Principal Component Analysis**. Given a large space, the PCA looks for the minimum number of axes that best represents the variation of the data.\n",
    "\n",
    "<img src=\"images/pca.PNG\" width=400, height=400>\n",
    "\n",
    "The eigenvectors of the covariance matrix define the axis of maximum variance and the eigenvalues give a measure of the variance of the data. \n",
    "\n",
    "1. Construct the vector in the (m x n)-dimensional space R given M images of size (m x n).\n",
    "\n",
    "2. Compute the mean image \n",
    "\n",
    "<center>\n",
    "$\\overline{X}=\\frac{1}{M}\\sum_{i=1}^{M} X_i$\n",
    "</center>\n",
    "\n",
    "3. Construct the covariance matrix. Due to $A \\times A^T$ is too large, instead of using $A \\times A^T$ to compute its eigenvectors, we are going to compute the eigenvectors of $A^T \\times A$.\n",
    "\n",
    "<img src=\"images/covariance_image.PNG\" width=500, height=500>\n",
    "\n",
    "4. Extract the eigenvectors (the base of the new space) and their eigenvalues and project faces in the new space to apply the classifier (knn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Load and prepare data for PCA\n",
    "\n",
    "Let's use the [Labeled Faces in the Wild (LFW)](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_lfw_people.html) people dataset (classification).\n",
    "\n",
    "Face dataset features:\n",
    "\n",
    "- Classes: 5749\n",
    "\n",
    "- Samples total: 13233\n",
    "\n",
    "- Dimensionality: 5828\n",
    "\n",
    "- Features: real, between 0 and 255\n",
    "\n",
    "\n",
    "*sklearn.datasets.fetch_lfw_people(data_home=None, funneled=True, resize=0.5, min_faces_per_person=0, color=False, slice_=(slice(70, 195, None), slice(78, 172, None)), download_if_missing=True, return_X_y=False)*\n",
    "\n",
    "\n",
    "\n",
    "*Please, check the parameters and returned value by ``lethc_lfw_people()`` before continuing.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8.1** Load the dataset, obtaining only those cases where there are, **at least, 100 images**. Check the final number of images, image shapes and labels of the images.\n",
    "\n",
    "*Hint: There is a parameter to set the minimum number of faces per person to load in the dataset function*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8.2** Plot an image frome each example, with its name as the title of the image.\n",
    "\n",
    "<img src=\"images/example.png\" width=500, height=500>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8.3** Divide the dataset into train and test set (0.7/0.3). Print the size of train and test image sets.\n",
    "\n",
    "Hint: use the train_test_split function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute PCA\n",
    "\n",
    "[*class sklearn.decomposition.PCA(n_components=None, copy=True, whiten=False, svd_solver='auto', tol=0.0, iterated_power='auto', random_state=None)*](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html?highlight=pca#sklearn.decomposition.PCA)\n",
    "\n",
    "The principal components measure deviations about this mean along orthogonal axes.\n",
    "\n",
    "**8.4** Create a PCA object, using the training set and 150 components. \n",
    "\n",
    "*Consider that PCA requires the data to be reshaped into 2 dimensions: number of observations x number of pixels*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the accumated variance of the components. \n",
    "\n",
    "**Hint:** Use `explained_variance_ratio_` to return the variance explained, plot this.\n",
    "\n",
    "<img src=\"images/explained_variance_example.jpg\" width=400, height=400>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8.5** One interesting part of PCA is that it computes the average face, which can be interesting to examine. \n",
    "\n",
    "Plot the average face, using the method `mean_` of the PCA object.\n",
    "\n",
    "**Hint:** The average face need to be reshaped in order to visualize it properly\n",
    "\n",
    "\n",
    "<img src=\"images/mean_face.jpg\" width=200, height=200>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8.6** Furhtermore, we can check all the principal components (i.e. eigenfaces) considering the corresponding importance. Visualize 30 principal eigenfaces.\n",
    "\n",
    "<img src=\"images/eigenfaces.png\" width=500, height=500>\n",
    "\n",
    "Note that the base components are ordered by their importance. We see that the first few components seem to primarily take care of lighting conditions; the remaining components pull out certain identifying features: the nose, eyes, eyebrows, etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8.7** Project both the training and test set onto the PCA basis, using the method `transform()` of the PCA object.\n",
    "\n",
    "These projected components correspond to factors in a linear combination of component images such that the combination approaches the original face. \n",
    "\n",
    "Choose one of the images and recompose from its first 10 most important corresponding eigenfaces. **Note that** we need to use the average face as the basis to agregate the rest of the components.\n",
    "\n",
    "\n",
    "<img src=\"images/eigenfaces_image.PNG\" width=300, height=300>\n",
    "\n",
    "Example of output: \n",
    "\n",
    "<img src=\"images/recomposition_example.jpg\" width=600, height=600>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a classifier\n",
    "\n",
    "**8.8** Train an Adaboost classifier using the PCA features. Show the results obtained with the test set.\n",
    "Use the `score` method of the Adaboost model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8.9** We can quantify this effectiveness using one of several measures from sklearn.metrics. First we can do the classification report, which shows the precision, recall and other measures of the ‚Äúgoodness‚Äù of the classification.\n",
    "\n",
    "*sklearn.metrics.classification_report(y_true, y_pred, labels=None, target_names=None, sample_weight=None, digits=2, output_dict=False, zero_division='warn')*\n",
    "\n",
    "*Please, check the parameters and returned value by ``classification_report()`` before continuing.*\n",
    "\n",
    "Print the classification report obtained during the training of the Adaboost model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recognize a new face example using the learned model\n",
    "\n",
    "**8.10** Try your model using the test set. \n",
    "\n",
    "Predict the labels using the Adaboost model, with and without PCA, and plot the images with the corresponding label as title.\n",
    "\n",
    "<img src=\"images/prediction.png\" width=300 height = 300>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix\n",
    "\n",
    "In case the library for exercise 1 does not work for you, here is the function that will create the filters for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Functions needed to create the Leung-Malik (LM) filter bank\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def gaussian1d(sigma, mean, x, ord):\n",
    "    x = np.array(x)\n",
    "    x_ = x - mean\n",
    "    var = sigma**2\n",
    "\n",
    "    # Gaussian Function\n",
    "    g1 = (1/np.sqrt(2*np.pi*var))*(np.exp((-1*x_*x_)/(2*var)))\n",
    "\n",
    "    if ord == 0:\n",
    "        g = g1\n",
    "        return g\n",
    "    elif ord == 1:\n",
    "        g = -g1*((x_)/(var))\n",
    "        return g\n",
    "    else:\n",
    "        g = g1*(((x_*x_) - var)/(var**2))\n",
    "        return g\n",
    "\n",
    "def gaussian2d(sup, scales):\n",
    "    var = scales * scales\n",
    "    shape = (sup,sup)\n",
    "    n,m = [(i - 1)/2 for i in shape]\n",
    "    x,y = np.ogrid[-m:m+1,-n:n+1]\n",
    "    g = (1/np.sqrt(2*np.pi*var))*np.exp( -(x*x + y*y) / (2*var) )\n",
    "    return g\n",
    "\n",
    "def log2d(sup, scales):\n",
    "    var = scales * scales\n",
    "    shape = (sup,sup)\n",
    "    n,m = [(i - 1)/2 for i in shape]\n",
    "    x,y = np.ogrid[-m:m+1,-n:n+1]\n",
    "    g = (1/np.sqrt(2*np.pi*var))*np.exp( -(x*x + y*y) / (2*var) )\n",
    "    h = g*((x*x + y*y) - var)/(var**2)\n",
    "    return h\n",
    "\n",
    "def makefilter(scale, phasex, phasey, pts, sup):\n",
    "\n",
    "    gx = gaussian1d(3*scale, 0, pts[0,...], phasex)\n",
    "    gy = gaussian1d(scale,   0, pts[1,...], phasey)\n",
    "\n",
    "    image = gx*gy\n",
    "\n",
    "    image = np.reshape(image,(sup,sup))\n",
    "    return image\n",
    "\n",
    "def makeLMfilters():\n",
    "    sup     = 49\n",
    "    scalex  = np.sqrt(2) * np.array([1,2,3])\n",
    "    norient = 6\n",
    "    nrotinv = 12\n",
    "\n",
    "    nbar  = len(scalex)*norient\n",
    "    nedge = len(scalex)*norient\n",
    "    nf    = nbar+nedge+nrotinv\n",
    "    F     = np.zeros([sup,sup,nf])\n",
    "    hsup  = (sup - 1)/2\n",
    "\n",
    "    x = [np.arange(-hsup,hsup+1)]\n",
    "    y = [np.arange(-hsup,hsup+1)]\n",
    "\n",
    "    [x,y] = np.meshgrid(x,y)\n",
    "\n",
    "    orgpts = [x.flatten(), y.flatten()]\n",
    "    orgpts = np.array(orgpts)\n",
    "\n",
    "    count = 0\n",
    "    for scale in range(len(scalex)):\n",
    "        for orient in range(norient):\n",
    "            angle = (np.pi * orient)/norient\n",
    "            c = np.cos(angle)\n",
    "            s = np.sin(angle)\n",
    "            rotpts = [[c+0,-s+0],[s+0,c+0]]\n",
    "            rotpts = np.array(rotpts)\n",
    "            rotpts = np.dot(rotpts,orgpts)\n",
    "            F[:,:,count] = makefilter(scalex[scale], 0, 1, rotpts, sup)\n",
    "            F[:,:,count+nedge] = makefilter(scalex[scale], 0, 2, rotpts, sup)\n",
    "            count = count + 1\n",
    "\n",
    "    count = nbar+nedge\n",
    "    scales = np.sqrt(2) * np.array([1,2,3,4])\n",
    "\n",
    "    for i in range(len(scales)):\n",
    "        F[:,:,count]   = gaussian2d(sup, scales[i])\n",
    "        count = count + 1\n",
    "\n",
    "    for i in range(len(scales)):\n",
    "        F[:,:,count] = log2d(sup, scales[i])\n",
    "        count = count + 1\n",
    "\n",
    "    for i in range(len(scales)):\n",
    "        F[:,:,count] = log2d(sup, 3*scales[i])\n",
    "        count = count + 1\n",
    "\n",
    "    return F"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
